###########################################################################
# Bisulphite sequencing (methylseq) analysis workflow
#
# This pipeline is adapted from current methyseq pipeline from BCB core
# This pipeline focuses on the second set of steps,
# Creator: Neelam Redekar, neelam.redekar@nih.gov
# Created: December 21, 2021
# Modifier: Tom Hill, tom.hill@nih.gov
# Modified: February 16, 2022
#
###########################################################################

from os.path import join
from snakemake.io import expand, glob_wildcards
from snakemake.utils import R
from os import listdir
import pandas as pd

#configfile: "/data/NHLBIcore/projects/NHLBI-16/MethylSnakemake/config.yaml"
#df = pd.read_csv("/data/NHLBIcore/projects/NHLBI-16/MethylSnakemake/samples.txt", header=0, sep='\t')

##
## Locations of working directories and reference genomes for analysis
##
sample_file= config["samples"]
rawdata_dir= config["rawdata_dir"]
working_dir= config["result_dir"]
hg38_fa= config["hg38_fa"]
phage_fa= config["phage_fa"]
bisulphite_genome_path= config["bisulphite_ref"]
bisulphite_fa= config["bisulphite_fa"]
species= config["species"]

REF_ATLAS=config["REF_ATLAS"]
CpG_MAP_TABLE=config["CpG_MAP_TABLE"]

##
## Read in the masterkey file for 3 tab-delimited columns of samples, groups and comparison
## Each sample can be in the file multiple times if used in multiple comparisons, but will only be mapped/process once.
##
## e.g.
##
##sample	group	comp
##S1	GA	GAvsGB
##S2	GA	GAvsGB
##S3	GB	GAvsGB
##S4	GB	GAvsGB
##S5	GC	GAvsGC
##S6	GC	GAvsGC
##S1	GA	GAvsGC
##S2	GA	GAvsGC

## The file requires these headings as they are used in multiple rules later on.

## Here we read in the samples file generated and begin processing the data, printing out the samples and group comparisons

df = pd.read_csv(sample_file, header=0, sep='\t')

SAMPLES=list(set(df['sample'].tolist()))
GROUPS=list(set(df['comp'].tolist()))

print(SAMPLES)
print(len(SAMPLES))
print(GROUPS)
print(len(GROUPS))

CHRS = ['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10','chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19','chr20','chr21','chr22','chr23','chrX']

RN = ['R1', 'R2']

rule All:
    input:
      #Deconvolution output
      expand(join(working_dir, "CpG_CSV/{samples}.csv"),samples=SAMPLES),
      expand(join(working_dir, "deconvolution_CSV/{samples}.csv"),samples=SAMPLES),
      expand(join(working_dir, "deconvolution_CSV/{samples}_deconv.log"),samples=SAMPLES),
      join(working_dir, "deconvolution_CSV/total.csv"),
      join(working_dir, "deconvolution_CSV/total_deconv_output.csv"),
      join(working_dir, "deconvolution_CSV/total_deconv_plot.png"),

rule get_CpG:
	input:
		join(working_dir, "CpG/{samples}.bedGraph"),
	output:
		join(working_dir, "CpG_CSV/{samples}.csv"),
	params:
		rname="get_CpG",
		cutoff=5,
		script_dir=join(working_dir,"scripts"),
		dir1=join(working_dir,"CpG_CSV"),
    		dir2=join(working_dir,"deconvolution_CSV"),
	shell:
		"""
		mkdir -p {params.dir1}
		mkdir -p {params.dir2}
		module load R
		Rscript {params.script_dir}/get_methy.R {input} {wildcards.samples} {params.cutoff} {output}
		"""


rule get_overlap_meth:
  input:
    join(working_dir, "deconvolution_CSV/{samples}.meth.csv"),
  output:
    join(working_dir, "deconvolution_CSV/{samples}.csv"),
  params:
    rname="get_overlap_meth",
    map_table=CpG_MAP_TABLE,
  run:
    df_ref=pd.read_csv(params.map_table,sep='\t',header=None)
    df_ref.columns=['chromosome','start','end','cgid']
    df_ref=df_ref.loc[(df_ref['chromosome'].isin(CHRS)),]
    dfm=pd.read_csv(input[0])
    dfm=pd.merge(df_ref,dfm,on=['chromosome','start','end'],how='inner')
    dfm=dfm.drop(labels=['chromosome','start','end'],axis=1)
    dfm=dfm.set_index('cgid')
    dfm.to_csv(output[0])

rule run_deconv:
  input:
    join(working_dir, "deconvolution_CSV/{samples}.csv"),
  output:
    join(working_dir, "deconvolution_CSV/{samples}_deconv.log"),
  params:
    script_dir=join(working_dir,"scripts"),
    dir=join(working_dir,"deconvolution_CSV"),
    rname="run_deconv",
    ref=REF_ATLAS,
  shell:
    """
      module load python
      cd {params.dir}
      python {params.script_dir}/deconvolve.py --atlas_path {params.ref} --plot --residuals {input}  > {output}  2>&1
    """

rule merge_tables:
  input:
    expand(join(working_dir, "deconvolution_CSV/{samples}.csv"),samples=SAMPLES),
  output:
    join(working_dir, "deconvolution_CSV/total.csv"),
  run:
    dfm=pd.read_csv(input[0])
    for f in input[1:]:
      df=pd.read_csv(f)
      dfm=pd.merge(dfm,df,on='cgid',how='outer')

    dfm.to_csv(output[0],index=False)

rule run_deconv_merged:
  input:
    join(working_dir, "deconvolution_CSV/total.csv"),
  output:
    join(working_dir, "deconvolution_CSV/total_deconv_output.csv"),
    join(working_dir, "deconvolution_CSV/total_deconv_plot.png"),
  params:
    ref=REF_ATLAS,
    dir=join(working_dir,"deconvolution_CSV"),
    script_dir=join(working_dir,"scripts"),
  shell:
    """
    module load python
    cd {params.dir}
    python {params.script_dir}/deconvolve.py --atlas_path {params.ref} --plot --residuals {input}
    """
